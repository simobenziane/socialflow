{
  "name": "03_Ingest_Validate_v17",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "w1-ingest",
        "responseMode": "responseNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        -240,
        112
      ],
      "id": "ec577bfa-d414-4642-8906-e9e70cae9403",
      "name": "Webhook Trigger",
      "webhookId": "w1-ingest",
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "jsCode": "/**\n * W1 v17.5: Load Config (Database)\n * Reads client config from SQLite database instead of client.yaml\n * Loads batch.yaml for batch-specific settings\n */\nconst fs = require('fs');\nconst yaml = require('js-yaml');\nconst Database = require('better-sqlite3');\n\nconsole.log('[W1] ═══════════════════════════════════════');\nconsole.log('[W1] Starting Ingest & Validate v17.5');\nconsole.log('[W1] ═══════════════════════════════════════');\n\nconst input = $input.first().json || {};\nconst CONFIG_BASE = '/data/clients/_config';\nconst SETTINGS_PATH = `${CONFIG_BASE}/settings.json`;\nconst ACTIVE_JOB_PATH = `${CONFIG_BASE}/active_job.json`;\nconst DB_PATH = `${CONFIG_BASE}/socialflow.db`;\n\nlet CLIENT, BATCH;\n\n// Priority: webhook payload > active_job.json\nif (input.client || input.body?.client) {\n  CLIENT = input.client || input.body?.client;\n  BATCH = input.batch || input.body?.batch;\n  console.log(`[W1] ℹ Using webhook payload: ${CLIENT}/${BATCH}`);\n} else {\n  try {\n    const activeJob = JSON.parse(fs.readFileSync(ACTIVE_JOB_PATH, 'utf8'));\n    CLIENT = activeJob.current?.client || activeJob.client;\n    BATCH = activeJob.current?.batch || activeJob.batch;\n    console.log(`[W1] ℹ Using active_job.json: ${CLIENT}/${BATCH}`);\n  } catch (e) {\n    console.log('[W1] ✗ [CONFIG_MISSING] Cannot read active_job.json');\n    return [{\n      json: {\n        _error: true,\n        error_code: 'CONFIG_MISSING',\n        error_message: `Cannot determine client/batch. Send {client, batch} in webhook or create ${ACTIVE_JOB_PATH}`,\n        _start_time: Date.now(),\n        _config_base: CONFIG_BASE\n      }\n    }];\n  }\n}\n\nif (!CLIENT || !BATCH) {\n  console.log('[W1] ✗ [CONFIG_MISSING] Missing client or batch');\n  return [{\n    json: {\n      _error: true,\n      error_code: 'CONFIG_MISSING',\n      error_message: 'Both client and batch are required',\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\n\n// Security: Validate slugs contain only safe characters (prevent path traversal)\nconst SAFE_SLUG = /^[a-zA-Z0-9_-]+$/;\nif (!SAFE_SLUG.test(CLIENT) || !SAFE_SLUG.test(BATCH)) {\n  console.log('[W1] ✗ [INVALID_SLUG] Invalid characters in client/batch slug');\n  return [{\n    json: {\n      _error: true,\n      error_code: 'INVALID_SLUG',\n      error_message: `Invalid slug format. CLIENT=${CLIENT}, BATCH=${BATCH}. Only alphanumeric, dash, and underscore allowed.`,\n      client: CLIENT,\n      batch: BATCH,\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\nconsole.log('[W1] ✓ Slug validation passed');\n\n// Load settings\nlet settings;\ntry {\n  if (!fs.existsSync(SETTINGS_PATH)) {\n    return [{\n      json: {\n        _error: true,\n        error_code: 'CONFIG_MISSING',\n        error_message: `settings.json not found at ${SETTINGS_PATH}`,\n        client: CLIENT,\n        batch: BATCH,\n        _start_time: Date.now(),\n        _config_base: CONFIG_BASE\n      }\n    }];\n  }\n  settings = JSON.parse(fs.readFileSync(SETTINGS_PATH, 'utf8'));\n  console.log('[W1] ✓ settings.json loaded');\n} catch (e) {\n  return [{\n    json: {\n      _error: true,\n      error_code: 'CONFIG_MISSING',\n      error_message: `Cannot parse settings.json: ${e.message}`,\n      client: CLIENT,\n      batch: BATCH,\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\n\n// Validate Cloudflare URL\nconst CLOUDFLARE_URL = settings.cloudflare_tunnel_url;\nif (!CLOUDFLARE_URL || CLOUDFLARE_URL.includes('PASTE-YOUR-TUNNEL') || CLOUDFLARE_URL.includes('your-tunnel')) {\n  console.log('[W1] ✗ [CONFIG_MISSING] Cloudflare URL not configured');\n  return [{\n    json: {\n      _error: true,\n      error_code: 'CONFIG_MISSING',\n      error_message: 'Cloudflare tunnel URL not configured! Run cloudflared and update settings.json',\n      client: CLIENT,\n      batch: BATCH,\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\n\nif (!CLOUDFLARE_URL.startsWith('https://')) {\n  return [{\n    json: {\n      _error: true,\n      error_code: 'CONFIG_MISSING',\n      error_message: 'cloudflare_tunnel_url must start with https://',\n      client: CLIENT,\n      batch: BATCH,\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\n\nconsole.log(`[W1] ✓ Cloudflare URL: ${CLOUDFLARE_URL.substring(0, 50)}...`);\n\n// Load Ollama config for image description\nconst OLLAMA_URL = settings.ollama?.url_docker || 'http://host.docker.internal:11434/api/generate';\nconst IMAGE_DESCRIBER_MODEL = settings.ollama?.models?.image_describer || settings.ollama?.model || 'llava:7b';\nconst DESCRIPTION_TIMEOUT = settings.caption_generation?.description_timeout_ms || 60000;\n\n// VLM resize settings\nconst VLM_RESIZE_MAX = settings.vlm?.resize_max_dimension || 1024;\nconst VLM_RESIZE_QUALITY = settings.vlm?.resize_quality || 85;\nconst VLM_RESIZE_ENABLED = settings.vlm?.resize_enabled !== false;\n\nconsole.log(`[W1] ✓ Image describer: ${IMAGE_DESCRIBER_MODEL}`);\nconsole.log(`[W1] ✓ VLM resize: ${VLM_RESIZE_ENABLED ? `enabled (max ${VLM_RESIZE_MAX}px, quality ${VLM_RESIZE_QUALITY})` : 'disabled'}`);\n\n// ========================================\n// v17.5: Load client from DATABASE instead of YAML\n// ========================================\nconst LOCAL_BASE = settings.paths.docker_base;\nconst clientRoot = `${LOCAL_BASE}/${CLIENT}`;\n\nlet clientConfig;\nlet db;\ntry {\n  db = new Database(DB_PATH, { readonly: true });\n\n  // Query client with account IDs\n  const client = db.prepare(`\n    SELECT c.*,\n           (SELECT late_account_id FROM accounts WHERE client_id = c.id AND platform = 'instagram' AND is_active = 1 LIMIT 1) as instagram_account_id,\n           (SELECT late_account_id FROM accounts WHERE client_id = c.id AND platform = 'tiktok' AND is_active = 1 LIMIT 1) as tiktok_account_id\n    FROM clients c\n    WHERE c.slug = ?\n  `).get(CLIENT);\n\n  if (!client) {\n    db.close();\n    console.log(`[W1] ✗ [CLIENT_NOT_FOUND] Client \"${CLIENT}\" not found in database`);\n    return [{\n      json: {\n        _error: true,\n        error_code: 'CLIENT_NOT_FOUND',\n        error_message: `Client not found: ${CLIENT}. Create the client via the UI first.`,\n        client: CLIENT,\n        batch: BATCH,\n        _start_time: Date.now(),\n        _config_base: CONFIG_BASE\n      }\n    }];\n  }\n\n  if (client.is_active !== 1) {\n    db.close();\n    console.log(`[W1] ✗ [CLIENT_INACTIVE] Client \"${CLIENT}\" is not active`);\n    return [{\n      json: {\n        _error: true,\n        error_code: 'CLIENT_NOT_FOUND',\n        error_message: `Client \"${CLIENT}\" is not active. Enable it in the UI.`,\n        client: CLIENT,\n        batch: BATCH,\n        _start_time: Date.now(),\n        _config_base: CONFIG_BASE\n      }\n    }];\n  }\n\n  // Build clientConfig object matching the old YAML structure\n  clientConfig = {\n    name: client.name,\n    slug: client.slug,\n    is_active: client.is_active === 1,\n    language: client.language || 'fr',\n    timezone: client.timezone || 'Europe/Paris',\n    brand: {\n      voice: client.brand_voice || '',\n      target_audience: client.brand_target_audience || '',\n      description: client.brand_description || ''\n    },\n    hashtags: JSON.parse(client.hashtags || '[]'),\n    platform_defaults: JSON.parse(client.platform_defaults || '{\"photos\":{\"feed\":\"ig\",\"story\":\"ig\"},\"videos\":{\"feed\":\"ig,tt\",\"story\":\"ig\"}}'),\n    policy: JSON.parse(client.policy || '{}'),\n    video_ai_captions: client.video_ai_captions || 0,\n    accounts: {\n      instagram: { late_account_id: client.instagram_account_id || '' },\n      tiktok: { late_account_id: client.tiktok_account_id || '' }\n    }\n  };\n\n  db.close();\n  console.log(`[W1] ✓ Client loaded from database: ${clientConfig.name}`);\n\n} catch (e) {\n  if (db) db.close();\n  console.log(`[W1] ✗ [DB_ERROR] Database error: ${e.message}`);\n  return [{\n    json: {\n      _error: true,\n      error_code: 'DB_ERROR',\n      error_message: `Database error loading client: ${e.message}`,\n      client: CLIENT,\n      batch: BATCH,\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\n\n// Load batch.yaml (still file-based - auto-create if missing)\nconst batchRoot = `${clientRoot}/${BATCH}`;\nconst batchYamlPath = `${batchRoot}/batch.yaml`;\n\nlet batchConfig;\ntry {\n  // Auto-create batch.yaml if missing\n  if (!fs.existsSync(batchYamlPath)) {\n    if (!fs.existsSync(batchRoot)) {\n      return [{\n        json: {\n          _error: true,\n          error_code: 'BATCH_NOT_FOUND',\n          error_message: `Batch folder not found: ${batchRoot}`,\n          client: CLIENT,\n          batch: BATCH,\n          _start_time: Date.now(),\n          _config_base: CONFIG_BASE\n        }\n      }];\n    }\n    // Auto-generate minimal batch.yaml\n    const today = new Date().toISOString().split('T')[0];\n    const minimalYaml = `# Auto-generated batch config (v17.5)\nname: \"${BATCH}\"\ndescription: \"Auto-created batch\"\n\nbrief: |\n  Content for ${BATCH} batch.\n\nhashtags:\n  - \"#${CLIENT.replace(/-/g, '')}\"\n\nschedule:\n  start_date: \"${today}\"\n  strategy:\n    type: \"daily\"\n    max_per_day: 1\n  distribution:\n    mode: \"photos_first\"\n  defaults:\n    feed_time: \"19:00\"\n    story_time: \"12:00\"\n\nslot: \"feed\"\n`;\n    fs.writeFileSync(batchYamlPath, minimalYaml);\n    console.log('[W1] ✓ Auto-created batch.yaml for ' + BATCH);\n  }\n  batchConfig = yaml.load(fs.readFileSync(batchYamlPath, 'utf8'));\n  console.log(`[W1] ✓ batch.yaml loaded: ${batchConfig.name || BATCH}`);\n} catch (e) {\n  return [{\n    json: {\n      _error: true,\n      error_code: 'YAML_PARSE_ERROR',\n      error_message: `Cannot parse batch.yaml: ${e.message}`,\n      client: CLIENT,\n      batch: BATCH,\n      _start_time: Date.now(),\n      _config_base: CONFIG_BASE\n    }\n  }];\n}\n\n// Helper to get default account ID\nfunction getDefaultAccountId(accounts, platform) {\n  if (!accounts || !accounts[platform]) return '';\n  const platformAccounts = accounts[platform];\n  if (Array.isArray(platformAccounts)) {\n    const defaultAcc = platformAccounts.find(a => a.is_default) || platformAccounts[0];\n    return defaultAcc?.late_account_id || '';\n  }\n  return platformAccounts.late_account_id || '';\n}\n\nconst INSTAGRAM_ACCOUNT_ID = getDefaultAccountId(clientConfig.accounts, 'instagram');\nconst TIKTOK_ACCOUNT_ID = getDefaultAccountId(clientConfig.accounts, 'tiktok');\nconst TIMEZONE = clientConfig.timezone || settings.defaults?.timezone || 'Europe/Berlin';\nconst LANGUAGE = clientConfig.language || settings.defaults?.language || 'fr';\nconst FEED_TIME = batchConfig.schedule?.defaults?.feed_time || settings.defaults?.feed_time || '20:00';\nconst STORY_TIME = batchConfig.schedule?.defaults?.story_time || settings.defaults?.story_time || '18:30';\n\nconst PLATFORM_DEFAULTS = batchConfig.platforms || clientConfig.platform_defaults || {\n  photos: { feed: 'ig', story: 'ig' },\n  videos: { feed: 'ig,tt', story: 'ig' }\n};\n\nconst POLICY = clientConfig.policy || {\n  require_video_frames: false,\n  video_frames_required: 0,\n  require_cover_image: false,\n  tiktok_video_only: true,\n  story_requires_story_asset: false\n};\n\n// Merge hashtags (client + batch)\nconst clientHashtags = clientConfig.hashtags || [];\nconst batchHashtags = batchConfig.hashtags || [];\nconst allHashtags = [...clientHashtags, ...batchHashtags]\n  .map(h => h.trim())\n  .filter(h => h.length > 0)\n  .map(h => h.startsWith('#') ? h : `#${h}`);\nconst uniqueHashtags = [...new Set(allHashtags.map(h => h.toLowerCase()))]\n  .map(lower => allHashtags.find(h => h.toLowerCase() === lower));\n\nconsole.log(`[W1] ✓ Hashtags: ${uniqueHashtags.length} (client: ${clientHashtags.length}, batch: ${batchHashtags.length})`);\n\n// Build schedule config\nconst schedule = {\n  start_date: batchConfig.schedule?.start_date || new Date().toISOString().split('T')[0],\n  timezone: TIMEZONE,\n  defaults: { feed_time: FEED_TIME, story_time: STORY_TIME },\n  strategy: {\n    type: batchConfig.schedule?.strategy?.type || 'daily',\n    interval_days: batchConfig.schedule?.strategy?.interval_days || 1,\n    weekdays: batchConfig.schedule?.strategy?.weekdays || [],\n    dates: batchConfig.schedule?.strategy?.dates || [],\n    max_per_day: batchConfig.schedule?.strategy?.max_per_day || 1,\n    distribution: { mode: batchConfig.schedule?.distribution?.mode || 'alternate' }\n  },\n  platforms: PLATFORM_DEFAULTS,\n  slot: batchConfig.slot || 'feed',\n  overrides: batchConfig.overrides || []\n};\n\nconsole.log(`[W1] ✓ Schedule: type=${schedule.strategy.type}, start=${schedule.start_date}`);\n\nconst mediaBaseUrl = `${CLOUDFLARE_URL}/${CLIENT}/${BATCH}`;\nconsole.log(`[W1] ℹ Batch root: ${batchRoot}`);\nconsole.log(`[W1] ℹ Media URL: ${mediaBaseUrl}`);\n\n// Get source_type from database if batch exists there\nlet SOURCE_TYPE = 'folder';\ntry {\n  const db2 = new Database(DB_PATH);\n  const clientRow = db2.prepare('SELECT id FROM clients WHERE slug = ?').get(CLIENT);\n  if (clientRow) {\n    const batchRow = db2.prepare('SELECT source_type FROM batches WHERE client_id = ? AND slug = ?').get(clientRow.id, BATCH);\n    if (batchRow && batchRow.source_type) {\n      SOURCE_TYPE = batchRow.source_type;\n      console.log(`[W1] ✓ Source type from database: ${SOURCE_TYPE}`);\n    }\n  }\n  db2.close();\n} catch (e) {\n  console.log(`[W1] ⚠ Could not check source_type: ${e.message}`);\n}\n\nreturn [{\n  json: {\n    client_slug: CLIENT,\n    batch_name: BATCH,\n    cloudflare_url: CLOUDFLARE_URL,\n    local_base: LOCAL_BASE,\n    batch_root: batchRoot,\n    client_root: clientRoot,\n    media_base_url: mediaBaseUrl,\n    ready_path: `${batchRoot}/READY.txt`,\n    photos_dir: `${batchRoot}/photos`,\n    videos_dir: `${batchRoot}/videos`,\n    instagram_account_id: INSTAGRAM_ACCOUNT_ID,\n    tiktok_account_id: TIKTOK_ACCOUNT_ID,\n    timezone: TIMEZONE,\n    language: LANGUAGE,\n    feed_time: FEED_TIME,\n    story_time: STORY_TIME,\n    platform_defaults: PLATFORM_DEFAULTS,\n    policy: POLICY,\n    db_path: DB_PATH,\n    source_type: SOURCE_TYPE,\n    schedule,\n    hashtags_list: uniqueHashtags,\n    hashtags_joined: uniqueHashtags.join(' '),\n    client_config: clientConfig,\n    batch_config: batchConfig,\n    brief: batchConfig.brief || '',\n    _start_time: Date.now(),\n    _source: input.body ? 'webhook' : 'manual',\n    _ingest_id: `ingest_${Date.now()}_${Math.random().toString(36).substr(2, 6)}`,\n    _config_base: CONFIG_BASE,\n    _ollama_url: OLLAMA_URL,\n    _image_describer_model: IMAGE_DESCRIBER_MODEL,\n    _description_timeout: DESCRIPTION_TIMEOUT,\n    _vlm_resize_enabled: VLM_RESIZE_ENABLED,\n    _vlm_resize_max: VLM_RESIZE_MAX,\n    _vlm_resize_quality: VLM_RESIZE_QUALITY,\n    generateVideoAI: (() => {\n      // v17.1: Resolve video_ai_captions setting (batch overrides client)\n      const clientVideoAI = clientConfig.video_ai_captions === 1;\n      const batchVideoAI = batchConfig?.video_ai_captions;\n      const result = batchVideoAI !== null && batchVideoAI !== undefined\n        ? batchVideoAI === 1\n        : clientVideoAI;\n      console.log('[W1] Video AI: client=' + clientVideoAI + ', batch=' + batchVideoAI + ', final=' + result);\n      return result;\n    })(),\n    generatePhotoAI: (() => {\n      // v17.6: Resolve photo_ai_captions setting (batch overrides client)\n      const clientPhotoAI = clientConfig.photo_ai_captions !== 0; // default true if undefined\n      const batchPhotoAI = batchConfig?.photo_ai_captions;\n      const result = batchPhotoAI !== null && batchPhotoAI !== undefined\n        ? batchPhotoAI === 1\n        : clientPhotoAI;\n      console.log('[W1] Photo AI: client=' + clientPhotoAI + ', batch=' + batchPhotoAI + ', final=' + result);\n      return result;\n    })(),\n    _error: false\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        0,
        0
      ],
      "id": "87f6a9da-867d-4ce8-b727-57d2adf40a43",
      "name": "Load Config (Database)"
    },
    {
      "parameters": {
        "jsCode": "/**\n * W1 v17: Validate Context\n * Checks READY.txt, brief, and database existence\n */\nconst fs = require('fs');\nconst crypto = require('crypto');\nconst config = $input.first().json;\n\nif (config._error) {\n return [{ json: config }];\n}\n\n// Check READY.txt exists (skip for upload batches - v17)\nif (config.source_type !== 'upload') {\n if (!fs.existsSync(config.ready_path)) {\n console.log('[W1] âœ— [READY_FILE_MISSING] READY.txt not found');\n return [{\n json: {\n ...config,\n _error: true,\n error_code: 'READY_FILE_MISSING',\n error_message: `READY.txt not found at ${config.ready_path}. Batch is not ready for processing.`\n }\n }];\n }\n console.log('[W1] âœ“ READY.txt found');\n} else {\n console.log('[W1] â„¹ Skipping READY.txt check (upload batch)');\n}\n\n// Validate brief\nif (!config.brief || config.brief.trim() === '') {\n console.log('[W1] âš  No brief found in batch.yaml, using default');\n config.brief = 'Content for social media. Professional, engaging tone.';\n}\n\nconst briefHash = crypto.createHash('md5').update(config.brief).digest('hex');\nconsole.log(`[W1] âœ“ Brief loaded (${config.brief.length} chars)`);\nconsole.log(`[W1] âœ“ Hashtags: ${config.hashtags_list.length} tags`);\n\n// Verify database exists\nif (!fs.existsSync(config.db_path)) {\n console.log('[W1] âœ— [DB_NOT_FOUND] Database not found');\n return [{\n json: {\n ...config,\n _error: true,\n error_code: 'DB_NOT_FOUND',\n error_message: `Database not found at ${config.db_path}. Run init_database.js first.`\n }\n }];\n}\nconsole.log('[W1] âœ“ Database found');\n\nreturn [{ json: { ...config, brief_hash: briefHash } }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        240,
        0
      ],
      "id": "f6247e92-57cf-4673-a80a-c372293ca075",
      "name": "Validate Context"
    },
    {
      "parameters": {
        "jsCode": "/**\n * W1 v17: Auto-Discover & Schedule\n * Discovers media files and assigns schedule dates\n * v15.3: Include batch name in content_id for batch isolation\n */\nconst fs = require('fs');\nconst crypto = require('crypto');\nconst ctx = $input.first().json;\n\nif (ctx._error) {\n return [{ json: ctx }];\n}\n\nconst PHOTO_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.webp'];\nconst VIDEO_EXTENSIONS = ['.mp4', '.mov', '.webm'];\nconst FRAME_PATTERN_NEW = /_F([1-4])$/i;\nconst FRAME_PATTERN_OLD = /__F([1-4])$/i;\nconst COVER_PATTERN_NEW = /_COVER$/i;\nconst COVER_PATTERN_OLD = /__COVER$/i;\nconst STORY_PATTERN_NEW = /_STORY$/i;\nconst STORY_PATTERN_OLD = /__STORY$/i;\n\nconst getExtension = (f) => {\n const parts = f.split('.');\n return parts.length > 1 ? '.' + parts.pop().toLowerCase() : '';\n};\n\nconst getBasename = (f) => {\n const parts = f.split('.');\n if (parts.length > 1) parts.pop();\n return parts.join('.');\n};\n\nconst isFrameFile = (b) => FRAME_PATTERN_NEW.test(b) || FRAME_PATTERN_OLD.test(b);\nconst isCoverFile = (b) => COVER_PATTERN_NEW.test(b) || COVER_PATTERN_OLD.test(b);\nconst isStoryFile = (b) => STORY_PATTERN_NEW.test(b) || STORY_PATTERN_OLD.test(b);\nconst getVideoBaseFromFrame = (b) => b.replace(/_F[1-4]$/i, '').replace(/__F[1-4]$/i, '');\nconst getVideoBaseFromCover = (b) => b.replace(/_COVER$/i, '').replace(/__COVER$/i, '');\n\n// v17: Check source_type for upload vs folder batches\nconst sourceType = ctx.source_type || 'folder';\nlet photos = [];\nlet videos = [];\nconst videoMap = new Map();\nconst deprecationWarnings = [];\n\nif (sourceType === 'upload') {\n // Query files table for upload-based batches\n try {\n const Database = require('better-sqlite3');\n const db = new Database(ctx.db_path);\n const clientRow = db.prepare('SELECT id FROM clients WHERE slug = ?').get(ctx.client_slug);\n const batchRow = db.prepare('SELECT id FROM batches WHERE client_id = ? AND slug = ?').get(clientRow?.id, ctx.batch_name);\n if (batchRow) {\n const files = db.prepare('SELECT id, original_name, storage_path, mime_type, file_size, checksum FROM files WHERE batch_id = ? AND status = ? ORDER BY uploaded_at').all(batchRow.id, 'ready');\n for (const file of files) {\n const isVideo = file.mime_type?.startsWith('video/');\n if (isVideo) {\n videos.push({ file: file.original_name, basename: file.original_name.replace(/\\\\.[^/.]+$/, ''), frames: [null, null, null, null], cover: null, story: null, _db_path: file.storage_path, _db_hash: file.checksum, _db_size: file.file_size, _file_id: file.id });\n } else {\n photos.push({ file: file.original_name, basename: file.original_name.replace(/\\\\.[^/.]+$/, ''), story: null, _db_path: file.storage_path, _db_hash: file.checksum, _db_size: file.file_size, _file_id: file.id });\n }\n }\n console.log(`[W1] âœ“ Loaded ${files.length} files from database (source_type=upload)`);\n }\n db.close();\n } catch (e) {\n console.log(`[W1] âœ— Error loading files from database: ${e.message}`);\n }\n}\n\n// Discover videos (folder-based batch)\nif (sourceType === 'folder') {\n\ntry {\n if (fs.existsSync(ctx.videos_dir)) {\n const allFiles = fs.readdirSync(ctx.videos_dir).sort();\n\n // First pass: find video files\n for (const file of allFiles) {\n const ext = getExtension(file);\n const basename = getBasename(file);\n if (VIDEO_EXTENSIONS.includes(ext) && !isStoryFile(basename)) {\n const video = { file, basename, frames: [null, null, null, null], cover: null, story: null };\n videos.push(video);\n videoMap.set(basename.toLowerCase(), video);\n }\n }\n\n // Second pass: associate frames, covers, stories\n for (const file of allFiles) {\n const ext = getExtension(file);\n const basename = getBasename(file);\n const fullPath = `${ctx.videos_dir}/${file}`;\n\n if (PHOTO_EXTENSIONS.includes(ext) && isFrameFile(basename)) {\n const videoBase = getVideoBaseFromFrame(basename).toLowerCase();\n const video = videoMap.get(videoBase);\n if (video) {\n const matchNew = basename.match(FRAME_PATTERN_NEW);\n const matchOld = basename.match(FRAME_PATTERN_OLD);\n if (matchNew) {\n video.frames[parseInt(matchNew[1]) - 1] = fullPath;\n } else if (matchOld) {\n video.frames[parseInt(matchOld[1]) - 1] = fullPath;\n deprecationWarnings.push(`DEPRECATED: Use _F${matchOld[1]} instead of __F${matchOld[1]} for ${file}`);\n }\n }\n } else if (PHOTO_EXTENSIONS.includes(ext) && isCoverFile(basename)) {\n const videoBase = getVideoBaseFromCover(basename).toLowerCase();\n const video = videoMap.get(videoBase);\n if (video) {\n video.cover = fullPath;\n if (COVER_PATTERN_OLD.test(basename)) {\n deprecationWarnings.push(`DEPRECATED: Use _COVER instead of __COVER for ${file}`);\n }\n }\n } else if (VIDEO_EXTENSIONS.includes(ext) && isStoryFile(basename)) {\n const mainBase = basename.replace(/_STORY$/i, '').replace(/__STORY$/i, '').toLowerCase();\n const video = videoMap.get(mainBase);\n if (video) {\n video.story = fullPath;\n if (STORY_PATTERN_OLD.test(basename)) {\n deprecationWarnings.push(`DEPRECATED: Use _STORY instead of __STORY for ${file}`);\n }\n }\n }\n }\n\n  // Auto-generate covers for videos without them (v17.7)\n  const { execSync } = require('child_process');\n  for (const [videoBase, videoObj] of videoMap) {\n    if (!videoObj.cover && videoObj.file) {\n      const videoPath = path.join(ctx.videos_dir, videoObj.file);\n      const coverName = videoObj.basename + '_COVER.jpg';\n      const coverPath = path.join(ctx.videos_dir, coverName);\n\n      try {\n        // Extract frame at 1 second (fallback to first frame if video < 1s)\n        execSync(`ffmpeg -y -i \"${videoPath}\" -ss 00:00:01 -vframes 1 -q:v 2 \"${coverPath}\" 2>/dev/null || ffmpeg -y -i \"${videoPath}\" -vframes 1 -q:v 2 \"${coverPath}\" 2>/dev/null`, {\n          timeout: 30000\n        });\n\n        if (fs.existsSync(coverPath)) {\n          videoObj.cover = coverPath;\n          console.log(`[W1] Generated cover: ${coverName}`);\n        }\n      } catch (err) {\n        console.warn(`[W1] Could not generate cover for ${videoBase}: ${err.message}`);\n      }\n    }\n  }\n\n  \n console.log(`[W1] âœ“ Discovered ${videos.length} videos`);\n } else {\n console.log(`[W1] âš  Videos directory not found: ${ctx.videos_dir}`);\n }\n} catch (e) {\n console.log(`[W1] âš  Error reading videos: ${e.message}`);\n}\n\n// Discover photos (folder-based batch)\ntry {\n if (fs.existsSync(ctx.photos_dir)) {\n const allFiles = fs.readdirSync(ctx.photos_dir).sort();\n\n for (const file of allFiles) {\n const ext = getExtension(file);\n const basename = getBasename(file);\n if (PHOTO_EXTENSIONS.includes(ext) && !isStoryFile(basename)) {\n photos.push({ file, basename, story: null });\n }\n }\n\n // Associate stories\n for (const file of allFiles) {\n const ext = getExtension(file);\n const basename = getBasename(file);\n if (PHOTO_EXTENSIONS.includes(ext) && isStoryFile(basename)) {\n const mainBase = basename.replace(/_STORY$/i, '').replace(/__STORY$/i, '').toLowerCase();\n const photo = photos.find(p => p.basename.toLowerCase() === mainBase);\n if (photo) {\n photo.story = `${ctx.photos_dir}/${file}`;\n if (STORY_PATTERN_OLD.test(basename)) {\n deprecationWarnings.push(`DEPRECATED: Use _STORY instead of __STORY for ${file}`);\n }\n }\n }\n }\n console.log(`[W1] âœ“ Discovered ${photos.length} photos`);\n } else {\n console.log(`[W1] âš  Photos directory not found: ${ctx.photos_dir}`);\n }\n} catch (e) {\n console.log(`[W1] âš  Error reading photos: ${e.message}`);\n}\n} // end if (sourceType === 'folder')\n\nif (deprecationWarnings.length > 0) {\n console.log(`[W1] âš  ${deprecationWarnings.length} deprecation warnings`);\n}\n\nif (photos.length === 0 && videos.length === 0) {\n console.log('[W1] âœ— [NO_ITEMS] No media files found');\n return [{\n json: {\n ...ctx,\n _error: true,\n error_code: 'NO_ITEMS',\n error_message: sourceType === 'upload' ? 'No ready files found in database for this batch. Ensure files were uploaded and processed.' : `No media files found in ${ctx.photos_dir} or ${ctx.videos_dir}. Ensure photos/ or videos/ directories contain valid media files.`\n }\n }];\n}\n\n// Scheduling functions\nfunction computeScheduleDates(schedule, totalItems) {\n const dates = [];\n let currentDate = new Date(schedule.start_date + 'T00:00:00');\n let itemsScheduled = 0;\n let iterations = 0;\n const maxIterations = 365;\n\n while (itemsScheduled < totalItems && iterations < maxIterations) {\n iterations++;\n let shouldPost = false;\n\n switch (schedule.strategy.type) {\n case 'daily': shouldPost = true; break;\n case 'interval': shouldPost = (iterations - 1) % schedule.strategy.interval_days === 0; break;\n case 'weekly': shouldPost = schedule.strategy.weekdays.includes(currentDate.getDay()); break;\n case 'specific_dates': shouldPost = schedule.strategy.dates.includes(currentDate.toISOString().split('T')[0]); break;\n default: shouldPost = true;\n }\n\n if (shouldPost) {\n const itemsToday = Math.min(schedule.strategy.max_per_day, totalItems - itemsScheduled);\n for (let i = 0; i < itemsToday; i++) {\n dates.push(currentDate.toISOString().split('T')[0]);\n itemsScheduled++;\n }\n }\n currentDate.setDate(currentDate.getDate() + 1);\n }\n return dates;\n}\n\nfunction sortFilesByDistribution(photos, videos, mode) {\n const items = [];\n switch (mode) {\n case 'photos_first':\n photos.forEach(p => items.push({ ...p, type: 'photo' }));\n videos.forEach(v => items.push({ ...v, type: 'video' }));\n break;\n case 'videos_first':\n videos.forEach(v => items.push({ ...v, type: 'video' }));\n photos.forEach(p => items.push({ ...p, type: 'photo' }));\n break;\n case 'alternate':\n const maxLen = Math.max(photos.length, videos.length);\n for (let i = 0; i < maxLen; i++) {\n if (i < photos.length) items.push({ ...photos[i], type: 'photo' });\n if (i < videos.length) items.push({ ...videos[i], type: 'video' });\n }\n break;\n default: // mixed\n photos.forEach(p => items.push({ ...p, type: 'photo' }));\n videos.forEach(v => items.push({ ...v, type: 'video' }));\n items.sort((a, b) => a.file.localeCompare(b.file));\n }\n return items;\n}\n\nfunction computeFileHash(filePath) {\n try {\n if (fs.existsSync(filePath)) {\n return crypto.createHash('md5').update(fs.readFileSync(filePath)).digest('hex');\n }\n } catch (e) {}\n return '';\n}\n\nfunction generateContentId(clientSlug, batchName, date, slot, mediaType, index) {\n const dateClean = date.replace(/-/g, '');\n const seq = String(index + 1).padStart(2, '0');\n return `${clientSlug}__${batchName}__${dateClean}__${slot}__${mediaType}__${seq}`;\n}\n\n// Build items\nconst sortedItems = sortFilesByDistribution(photos, videos, ctx.schedule.strategy.distribution?.mode || 'alternate');\nconst scheduleDates = computeScheduleDates(ctx.schedule, sortedItems.length);\nconst allItems = [];\n\nfor (let i = 0; i < sortedItems.length; i++) {\n const mediaItem = sortedItems[i];\n const isPhoto = mediaItem.type === 'photo';\n const override = (ctx.schedule.overrides || []).find(o => o.file === mediaItem.file) || {};\n\n const dir = isPhoto ? ctx.photos_dir : ctx.videos_dir;\n // v17: Use _db_path for upload batches, otherwise construct from dir\n const filePath = mediaItem._db_path || `${dir}/${mediaItem.file}`;\n const fileHash = mediaItem._db_hash || computeFileHash(filePath);\n\n const date = override.date || scheduleDates[i] || ctx.schedule.start_date;\n const slot = override.slot || ctx.schedule.slot || 'feed';\n // Determine time based on media type AND slot (v17.8)\n  const time = override.time || (() => {\n    if (slot === 'story') {\n      return ctx.schedule.story_time || ctx.schedule.defaults?.story_time || '12:00';\n    } else if (isPhoto) {\n      return ctx.schedule.photo_time || ctx.schedule.defaults?.feed_time || '19:00';\n    } else {\n      // Video/Reel\n      return ctx.schedule.video_time || ctx.schedule.defaults?.feed_time || '20:00';\n    }\n  })();\n\n let platforms = override.platforms;\n if (!platforms) {\n const platConfig = ctx.schedule.platforms || ctx.platform_defaults;\n platforms = isPhoto ? platConfig.photos?.[slot] : platConfig.videos?.[slot];\n }\n platforms = (platforms || 'ig').toLowerCase().replace(/tiktok/g, 'tt').replace(/instagram/g, 'ig');\n\n const contentId = generateContentId(ctx.client_slug, ctx.batch_name, date, slot, mediaItem.type, i);\n const mediaFolder = isPhoto ? 'photos' : 'videos';\n const cacheBuster = fileHash ? `?v=${fileHash.substring(0, 8)}` : `?t=${Date.now()}`;\n const mediaUrl = `${ctx.media_base_url}/${mediaFolder}/${mediaItem.file}${cacheBuster}`;\n\n const item = {\n content_id: contentId,\n client_slug: ctx.client_slug,\n batch_name: ctx.batch_name,\n media_type: mediaItem.type,\n file_name: mediaItem.file,\n file_path: filePath,\n media_url: mediaUrl,\n file_hash: fileHash,\n file_size: mediaItem._db_size || (fs.existsSync(filePath) ? fs.statSync(filePath).size : 0),\n file_id: mediaItem._file_id || null,\n frames_used: isPhoto ? '' : (mediaItem.frames || []).filter(f => f !== null).join(';'),\n cover_path: isPhoto ? '' : (mediaItem.cover || ''),\n story_path: mediaItem.story || '',\n scheduled_date: date,\n scheduled_time: time,\n schedule_at: `${date}T${time}:00`,\n timezone: ctx.schedule.timezone || ctx.timezone,\n slot,\n platforms,\n caption_override: override.caption_override || '',\n caption_ig: '',\n caption_tt: '',\n hashtags_final: ctx.hashtags_joined,\n notes: override.notes || '',\n status: 'PENDING',\n error_message: '',\n retry_count: 0,\n late_media_id: '',\n late_media_url: '',\n late_post_id: '',\n instagram_account_id: ctx.instagram_account_id,\n tiktok_account_id: ctx.tiktok_account_id,\n fingerprint: '',\n preview_url: mediaUrl,\n ingest_id: ctx._ingest_id,\n image_description: '',\n description_generated_at: null,\n _ctx: ctx,\n _frames: isPhoto ? [] : mediaItem.frames,\n _cover: isPhoto ? null : mediaItem.cover,\n _deprecation_warnings: deprecationWarnings,\n _item_index: i + 1,\n _total_items: sortedItems.length\n };\n\n const fpData = `${item.client_slug}|${item.batch_name}|${item.content_id}|${item.schedule_at}|${item.platforms}|${item.file_hash}`;\n item.fingerprint = crypto.createHash('sha256').update(fpData).digest('hex').substring(0, 16);\n allItems.push(item);\n}\n\nconsole.log(`[W1] âœ“ Built ${allItems.length} items`);\nif (scheduleDates.length > 0) {\n console.log(`[W1] â„¹ Date range: ${scheduleDates[0]} to ${scheduleDates[scheduleDates.length - 1]}`);\n} else {\n console.log(`[W1] â„¹ Using start_date fallback: ${ctx.schedule.start_date}`);\n}\n\n// v15.2: Initialize ingest progress tracking\ntry {\n const Database = require('better-sqlite3');\n const db = new Database(ctx.db_path);\n const clientRow = db.prepare('SELECT id FROM clients WHERE slug = ?').get(ctx.client_slug);\n if (clientRow) {\n const progress = JSON.stringify({\n current: 0,\n total: allItems.length,\n stage: 'discovering',\n started_at: new Date().toISOString()\n });\n db.prepare('UPDATE batches SET ingest_progress = ?, ingest_started_at = datetime(\\'now\\') WHERE client_id = ? AND slug = ?')\n .run(progress, clientRow.id, ctx.batch_name);\n console.log('[W1] âœ“ Ingest progress initialized');\n }\n db.close();\n} catch (e) {\n console.log(`[W1] âš  Could not initialize progress: ${e.message}`);\n}\n\nreturn allItems.map(item => ({ json: item }));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        480,
        0
      ],
      "id": "0e7d3373-00e2-4f29-a9c5-dc970d1fe77f",
      "name": "Auto-Discover & Schedule"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * W1 v17: Validate Media\n * Validates each media item (file existence, frames, etc.)\n * Added progress tracking\n */\nconst fs = require('fs');\nconst crypto = require('crypto');\nconst item = $json;\n\nif (item._error) {\n return { json: item };\n}\n\n// Update progress to validating stage\ntry {\n const Database = require('better-sqlite3');\n const db = new Database(item._ctx.db_path);\n const clientRow = db.prepare('SELECT id FROM clients WHERE slug = ?').get(item.client_slug);\n if (clientRow) {\n const progress = JSON.stringify({\n current: item._item_index,\n total: item._total_items,\n stage: 'validating',\n file_name: item.file_name,\n started_at: new Date().toISOString()\n });\n db.prepare('UPDATE batches SET ingest_progress = ? WHERE client_id = ? AND slug = ?')\n .run(progress, clientRow.id, item.batch_name);\n }\n db.close();\n} catch (e) {\n // Progress update failure shouldn't block validation\n}\n\nconst policy = item._ctx.policy;\nconst errors = [];\nconst warnings = [];\nconst itemErrors = [];\nlet status = 'NEEDS_AI';\n\n// v17.1: Configurable video AI caption generation\n// Stories always skip AI, videos check the generateVideoAI setting from _ctx\nif (item.slot === 'story') {\n status = 'NEEDS_REVIEW';\n console.log(`[W1] Setting ${item.file_name} to NEEDS_REVIEW (story - no caption needed)`);\n} else if (item.media_type === 'video' && !item._ctx.generateVideoAI) {\n status = 'NEEDS_REVIEW';\n console.log(`[W1] Setting ${item.file_name} to NEEDS_REVIEW (video AI disabled)`);\n} else if (item.media_type === 'photo' && !item._ctx.generatePhotoAI) {\n status = 'NEEDS_REVIEW';\n console.log(`[W1] Setting ${item.file_name} to NEEDS_REVIEW (photo AI disabled)`);\n}\n\n// File exists check\ntry {\n if (!fs.existsSync(item.file_path)) {\n errors.push(`File not found: ${item.file_name}`);\n itemErrors.push({ code: 'MEDIA_NOT_FOUND', message: `File not found: ${item.file_name}` });\n } else {\n const buffer = fs.readFileSync(item.file_path);\n const currentHash = crypto.createHash('md5').update(buffer).digest('hex');\n if (item.file_hash && currentHash !== item.file_hash) {\n warnings.push('File hash changed during processing');\n item.file_hash = currentHash;\n const baseUrl = item.media_url.split('?')[0];\n item.media_url = `${baseUrl}?v=${currentHash.substring(0, 8)}`;\n item.preview_url = item.media_url;\n }\n }\n} catch (e) {\n errors.push(`Cannot access file: ${item.file_name} - ${e.message}`);\n itemErrors.push({ code: 'MEDIA_NOT_FOUND', message: `Cannot access: ${e.message}` });\n}\n\n// Date validation\nif (!item.scheduled_date || !/^\\d{4}-\\d{2}-\\d{2}$/.test(item.scheduled_date)) {\n errors.push(`Invalid date format: ${item.scheduled_date || 'empty'}`);\n itemErrors.push({ code: 'CONFIG_MISSING', message: `Invalid date: ${item.scheduled_date}` });\n}\n\n// Story slot validation\nif (policy.story_requires_story_asset && item.slot === 'story' && !item.story_path) {\n warnings.push('Story slot but no _STORY variant found');\n}\n\n// TikTok requires video\nif (policy.tiktok_video_only && item.platforms.includes('tt') && item.media_type === 'photo') {\n errors.push('TikTok requires video content');\n itemErrors.push({ code: 'CONFIG_MISSING', message: 'TikTok requires video, not photo' });\n}\n\n// Video frames validation\nif (item.media_type === 'video' && policy.require_video_frames) {\n const frames = item._frames || [];\n const foundCount = frames.filter(f => f !== null).length;\n const required = policy.video_frames_required || 4;\n\n if (foundCount < required) {\n const missingFrames = [];\n for (let i = 0; i < required; i++) {\n if (!frames[i]) missingFrames.push(`_F${i + 1}`);\n }\n const msg = `Missing video frames: ${foundCount}/${required}. Need: ${missingFrames.join(', ')}`;\n errors.push(msg);\n itemErrors.push({ code: 'FRAMES_MISSING', message: msg });\n console.log(`[W1] âœ— [FRAMES_MISSING] ${item.file_name}: ${missingFrames.join(', ')}`);\n }\n}\n\n// Cover image check\nif (item.media_type === 'video' && policy.require_cover_image && !item._cover) {\n warnings.push('No cover image found');\n itemErrors.push({ code: 'COVER_MISSING', message: 'No _COVER image found' });\n}\n\n// File size check\nif (item.file_size > 0) {\n const sizeMB = item.file_size / (1024 * 1024);\n if (item.media_type === 'photo' && sizeMB > 8) warnings.push(`Large photo file (${sizeMB.toFixed(1)}MB)`);\n if (item.media_type === 'video' && sizeMB > 500) warnings.push(`Very large video file (${sizeMB.toFixed(1)}MB)`);\n}\n\n// Caption override check\nif (item.caption_override && item.caption_override.trim() !== '') {\n status = 'NEEDS_REVIEW';\n}\n\n// Set final status\nif (errors.length > 0) status = 'BLOCKED';\n\nconst allMessages = [...errors];\nif (warnings.length > 0) allMessages.push(`WARNINGS: ${warnings.join('; ')}`);\n\nreturn {\n json: {\n ...item,\n status,\n error_message: allMessages.join('; '),\n validated_at: new Date().toISOString(),\n _item_errors: itemErrors\n }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        720,
        0
      ],
      "id": "5280f3da-be9f-43e4-a2f9-39c6ad9c36e1",
      "name": "Validate Media"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * W1 v17: Prepare Image for VLM\n * Loads photo as binary for resize, skips videos\n * v15.1: Output binary data for Edit Image node\n */\nconst fs = require('fs');\nconst path = require('path');\nconst item = $json;\n\n// Skip if error or already has description\nif (item._error || item.status === 'BLOCKED') {\n return { json: { ...item, _skip_vlm: true } };\n}\n\n// Skip videos - they use frames which will be analyzed in W2\nif (item.media_type === 'video') {\n console.log(`[W1] â„¹ Skipping VLM for video: ${item.file_name}`);\n return { json: { ...item, _skip_vlm: true, image_description: '', description_generated_at: null } };\n}\n\n// Load image as binary for photos\ntry {\n if (!fs.existsSync(item.file_path)) {\n console.log(`[W1] âš  Image file not found, skipping VLM: ${item.file_path}`);\n return { json: { ...item, _skip_vlm: true, image_description: '', description_generated_at: null } };\n }\n \n const buffer = fs.readFileSync(item.file_path);\n const ext = path.extname(item.file_name).toLowerCase();\n const mimeType = ext === '.png' ? 'image/png' : ext === '.webp' ? 'image/webp' : 'image/jpeg';\n \n // Get original dimensions for logging\n const fileSizeKB = Math.round(buffer.length / 1024);\n console.log(`[W1] âœ“ Loaded image (${item._item_index}/${item._total_items}): ${item.file_name} (${fileSizeKB}KB)`);\n \n // Return with binary data for Edit Image node\n return {\n json: {\n ...item,\n _skip_vlm: false,\n _vlm_url: item._ctx._ollama_url,\n _vlm_timeout: item._ctx._description_timeout,\n _vlm_model: item._ctx._image_describer_model,\n _vlm_retry_count: 0,\n _original_size_kb: fileSizeKB\n },\n binary: {\n data: {\n data: buffer.toString('base64'),\n mimeType: mimeType,\n fileName: item.file_name\n }\n }\n };\n} catch (e) {\n console.log(`[W1] âš  Error loading image: ${e.message}`);\n return { json: { ...item, _skip_vlm: true, image_description: '', description_generated_at: null } };\n}"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        960,
        0
      ],
      "id": "prepare-vlm-request",
      "name": "Prepare Image for VLM"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "skip-vlm-check",
              "leftValue": "={{ $json._skip_vlm }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        1180,
        0
      ],
      "id": "should-skip-vlm",
      "name": "Skip VLM?"
    },
    {
      "parameters": {
        "operation": "resize",
        "dataPropertyName": "data",
        "width": 1024,
        "height": 1024,
        "resizeOption": "onlyIfLarger",
        "options": {
          "format": "jpeg",
          "quality": 85
        }
      },
      "type": "n8n-nodes-base.editImage",
      "typeVersion": 1,
      "position": [
        1400,
        -120
      ],
      "id": "resize-for-vlm",
      "name": "Resize for VLM"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * W1 v17: Build VLM Request\n * Converts resized binary image to base64 for Ollama\n * n8n Edit Image outputs binary in $binary.data with base64 in .data property\n * Added progress tracking\n * Language-specific prompts\n */\nconst item = $json;\nconst binary = $binary;\n\n// Update progress to describing stage\ntry {\n const Database = require('better-sqlite3');\n const db = new Database(item._ctx.db_path);\n const clientRow = db.prepare('SELECT id FROM clients WHERE slug = ?').get(item.client_slug);\n if (clientRow) {\n const progress = JSON.stringify({\n current: item._item_index,\n total: item._total_items,\n stage: 'describing',\n file_name: item.file_name,\n started_at: new Date().toISOString()\n });\n db.prepare('UPDATE batches SET ingest_progress = ? WHERE client_id = ? AND slug = ?')\n .run(progress, clientRow.id, item.batch_name);\n }\n db.close();\n} catch (e) {\n // Progress update failure shouldn't block VLM\n}\n\n// Debug: log what we received\nconsole.log(`[W1] Binary keys: ${binary ? Object.keys(binary).join(', ') : 'null'}`);\nif (binary && binary.data) {\n console.log(`[W1] Binary.data keys: ${Object.keys(binary.data).join(', ')}`);\n}\n\n// n8n binary format: $binary.data.data contains base64 string\nlet imageBase64 = '';\nif (binary && binary.data) {\n // Try different property names that n8n might use\n imageBase64 = binary.data.data || binary.data.base64 || '';\n \n // If still empty, try to convert from Buffer if present\n if (!imageBase64 && binary.data.buffer) {\n imageBase64 = Buffer.from(binary.data.buffer).toString('base64');\n }\n}\n\nif (!imageBase64) {\n console.log(`[W1] âš  No binary data for VLM request: ${item.content_id}`);\n console.log(`[W1] Binary structure: ${JSON.stringify(binary, null, 2).substring(0, 500)}`);\n return { json: { ...item, _skip_vlm: true, image_description: '' } };\n}\n\nconst resizedSizeKB = Math.round(imageBase64.length * 0.75 / 1024);\nconsole.log(`[W1] âœ“ Resized for VLM: ${item.file_name} (${item._original_size_kb || '?'}KB â†’ ${resizedSizeKB}KB)`);\n\n// Language-specific prompts\nconst language = item._ctx?.language || 'fr';\nconsole.log(`[W1] â„¹ Using language: ${language}`);\n\n// Build VLM prompt in client's language\nconst prompts = {\n fr: `USER: <image>\\nDÃ©cris cette image de maniÃ¨re objective en 2-3 phrases pour un contexte de rÃ©seaux sociaux. Inclus :\\n- Le(s) sujet(s) principal(aux) et leur apparence\\n- Le cadre/environnement et l'atmosphÃ¨re\\n- Les couleurs, l'Ã©clairage, l'ambiance ou l'Ã©motion transmise\\n- Toute action ou interaction notable\\n\\nConcentre-toi sur les dÃ©tails qui inspireraient des lÃ©gendes engageantes. Sois factuel mais capture la qualitÃ© Ã©motionnelle.\\nN'Ã©cris PAS de lÃ©gendes ni de texte marketing.\\n\\nRÃ©ponds en franÃ§ais.\\n\\nA:`,\n en: `USER: <image>\\nDescribe this image objectively in 2-3 sentences for social media context. Include:\\n- Main subject(s) and their appearance\\n- Setting/environment and atmosphere\\n- Colors, lighting, mood or feeling conveyed\\n- Any notable action or interaction\\n\\nFocus on details that would inspire engaging captions. Be factual but capture the emotional quality.\\nDo NOT write captions or suggest marketing text.\\n\\nA:`,\n de: `USER: <image>\\nBeschreibe dieses Bild objektiv in 2-3 SÃ¤tzen fÃ¼r einen Social-Media-Kontext. Beinhalte:\\n- Hauptmotiv(e) und deren Erscheinung\\n- Umgebung/Setting und AtmosphÃ¤re\\n- Farben, Beleuchtung, vermittelte Stimmung oder Emotion\\n- Bemerkenswerte Aktionen oder Interaktionen\\n\\nKonzentriere dich auf Details, die zu ansprechenden Bildunterschriften inspirieren. Sei sachlich, aber erfasse die emotionale QualitÃ¤t.\\nSchreibe KEINE Bildunterschriften oder Marketing-Texte.\\n\\nAntworte auf Deutsch.\\n\\nA:`\n};\n\nconst vlmPrompt = prompts[language] || prompts.fr;\n\nreturn {\n json: {\n ...item,\n _vlm_request: {\n model: item._vlm_model,\n prompt: vlmPrompt,\n images: [imageBase64],\n stream: false,\n options: {\n temperature: 0.5,\n num_predict: 200\n }\n },\n _resized_size_kb: resizedSizeKB\n }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1620,
        -120
      ],
      "id": "build-vlm-request",
      "name": "Build VLM Request"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $json._vlm_url }}",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify($json._vlm_request) }}",
        "options": {
          "timeout": "={{ $json._vlm_timeout || 60000 }}"
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1840,
        -120
      ],
      "id": "call-vlm",
      "name": "Call VLM",
      "onError": "continueRegularOutput"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * W1 v17: Handle VLM Response\n * Process VLM response with retry logic\n */\nconst prevItem = $('Build VLM Request').item;\nconst response = $json;\n\nif (!prevItem || !prevItem.json) {\n return { json: { _error: true, error_code: 'MISSING_DATA' } };\n}\n\nconst item = prevItem.json;\nconst config = item._ctx;\n\n// Check for errors\nif (response.error || !response.response) {\n const errorMsg = response.error || 'No response from VLM';\n console.log(`[W1] âš  VLM error for ${item.content_id}: ${errorMsg}`);\n \n const retryCount = item._vlm_retry_count || 0;\n const maxRetries = config?.caption_generation?.vlm_max_retries ?? 2;\n \n if (retryCount < maxRetries) {\n // Queue for retry\n console.log(`[W1] â„¹ Queuing VLM retry ${retryCount + 1}/${maxRetries} for ${item.content_id}`);\n return { \n json: { \n ...item, \n _vlm_retry_count: retryCount + 1,\n _vlm_needs_retry: true,\n image_description: '',\n description_generated_at: null\n } \n };\n }\n \n // Max retries reached - continue without description\n console.log(`[W1] âš  Max VLM retries (${maxRetries}) reached for ${item.content_id}, continuing without description`);\n return { \n json: { \n ...item, \n _vlm_needs_retry: false,\n image_description: '',\n description_generated_at: null,\n _vlm_error: errorMsg\n } \n };\n}\n\n// Parse successful response\nlet description = (response.response || '').trim();\n\n// Clean up common VLM artifacts\ndescription = description\n .replace(/^(The image shows|This image shows|I see|In this image,?)\\s*/i, '')\n .replace(/^(Here is|Here's)\\s+.*?:\\s*/i, '')\n .replace(/\\n+/g, ' ')\n .trim();\n\nconsole.log(`[W1] âœ“ VLM description for ${item.content_id}: ${description.substring(0, 80)}...`);\n\nreturn {\n json: {\n ...item,\n _vlm_needs_retry: false,\n image_description: description,\n description_generated_at: new Date().toISOString()\n }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2060,
        -120
      ],
      "id": "handle-vlm-response",
      "name": "Handle VLM Response"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "needs-retry-check",
              "leftValue": "={{ $json._vlm_needs_retry }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        2280,
        -120
      ],
      "id": "needs-vlm-retry",
      "name": "Needs Retry?"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * W1 v17: Prepare VLM Retry\n * Rebuild request for retry with resized image\n * Language-specific prompts\n */\nconst fs = require('fs');\nconst item = $json;\n\nconsole.log(`[W1] â„¹ Retrying VLM for ${item.content_id}`);\n\n// Reload and resize image\nlet imageBase64 = '';\ntry {\n if (fs.existsSync(item.file_path)) {\n const buffer = fs.readFileSync(item.file_path);\n // Note: For retry we use original - resize happens in main path\n // This is a simplified retry that just reloads the original\n imageBase64 = buffer.toString('base64');\n }\n} catch (e) {\n console.log(`[W1] âš  Error reloading image for retry: ${e.message}`);\n return { json: { ...item, _vlm_needs_retry: false, image_description: '' } };\n}\n\n// Language-specific prompts\nconst language = item._ctx?.language || 'fr';\nconst prompts = {\n fr: `USER: <image>\\nDÃ©cris cette image de maniÃ¨re objective en 2-3 phrases pour un contexte de rÃ©seaux sociaux. Inclus :\\n- Le(s) sujet(s) principal(aux) et leur apparence\\n- Le cadre/environnement et l'atmosphÃ¨re\\n- Les couleurs, l'Ã©clairage, l'ambiance ou l'Ã©motion transmise\\n- Toute action ou interaction notable\\n\\nConcentre-toi sur les dÃ©tails qui inspireraient des lÃ©gendes engageantes. Sois factuel mais capture la qualitÃ© Ã©motionnelle.\\nN'Ã©cris PAS de lÃ©gendes ni de texte marketing.\\n\\nRÃ©ponds en franÃ§ais.\\n\\nA:`,\n en: `USER: <image>\\nDescribe this image objectively in 2-3 sentences for social media context. Include:\\n- Main subject(s) and their appearance\\n- Setting/environment and atmosphere\\n- Colors, lighting, mood or feeling conveyed\\n- Any notable action or interaction\\n\\nFocus on details that would inspire engaging captions. Be factual but capture the emotional quality.\\nDo NOT write captions or suggest marketing text.\\n\\nA:`,\n de: `USER: <image>\\nBeschreibe dieses Bild objektiv in 2-3 SÃ¤tzen fÃ¼r einen Social-Media-Kontext. Beinhalte:\\n- Hauptmotiv(e) und deren Erscheinung\\n- Umgebung/Setting und AtmosphÃ¤re\\n- Farben, Beleuchtung, vermittelte Stimmung oder Emotion\\n- Bemerkenswerte Aktionen oder Interaktionen\\n\\nKonzentriere dich auf Details, die zu ansprechenden Bildunterschriften inspirieren. Sei sachlich, aber erfasse die emotionale QualitÃ¤t.\\nSchreibe KEINE Bildunterschriften oder Marketing-Texte.\\n\\nAntworte auf Deutsch.\\n\\nA:`\n};\nconst vlmPrompt = prompts[language] || prompts.fr;\n\nreturn {\n json: {\n ...item,\n _vlm_request: {\n model: item._vlm_model,\n prompt: vlmPrompt,\n images: [imageBase64],\n stream: false,\n options: { temperature: 0.5, num_predict: 200 }\n }\n }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2500,
        -240
      ],
      "id": "prepare-vlm-retry",
      "name": "Prepare VLM Retry"
    },
    {
      "parameters": {
        "mode": "append"
      },
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3.1,
      "position": [
        2720,
        0
      ],
      "id": "merge-vlm-results",
      "name": "Merge VLM Results"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "/**\n * W1 v17: Clean Item for DB\n * Remove internal fields before database insert\n */\nconst item = $json;\n\n// Remove internal VLM fields but keep image_description\nconst { \n _ctx, _frames, _cover, _deprecation_warnings, \n _vlm_request, _vlm_url, _vlm_timeout, _vlm_retry_count, _vlm_needs_retry, _vlm_error,\n _vlm_model, _original_size_kb, _resized_size_kb,\n _skip_vlm,\n ...cleanItem \n} = item;\n\nreturn {\n json: {\n ...cleanItem,\n _db_path: _ctx?.db_path || item._db_path,\n _client_config: _ctx?.client_config || item._client_config,\n _batch_config: _ctx?.batch_config || item._batch_config\n }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2940,
        0
      ],
      "id": "clean-for-db",
      "name": "Clean for DB"
    },
    {
      "parameters": {
        "jsCode": "/**\n * W1 v17: Upsert to SQLite\n * Creates/updates client, batch, and content item records\n * v15: Added image_description and description_generated_at fields\n */\nconst Database = require('better-sqlite3');\nconst items = $input.all();\n\nif (items.length === 1 && items[0].json._error) {\n return items;\n}\n\nif (items.length === 0) {\n throw new Error('No items to insert');\n}\n\nconst firstItem = items[0].json;\nconst dbPath = firstItem._db_path;\nconst clientConfig = firstItem._client_config;\nconst batchConfig = firstItem._batch_config;\n\nconsole.log(`[W1] â„¹ Opening database: ${dbPath}`);\nlet db = null;\n\ntry {\n db = new Database(dbPath);\n db.pragma('foreign_keys = ON');\n\n // Upsert client\n const upsertClient = db.prepare(`\n INSERT INTO clients (slug, name, is_active, language, timezone, brand_voice, brand_target_audience, brand_description, hashtags, platform_defaults, policy)\n VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n ON CONFLICT(slug) DO UPDATE SET\n name = excluded.name, is_active = excluded.is_active, language = excluded.language, timezone = excluded.timezone,\n brand_voice = excluded.brand_voice, brand_target_audience = excluded.brand_target_audience, brand_description = excluded.brand_description,\n hashtags = excluded.hashtags, platform_defaults = excluded.platform_defaults, policy = excluded.policy\n `);\n\n upsertClient.run(\n firstItem.client_slug,\n clientConfig.name || firstItem.client_slug,\n clientConfig.is_active !== false ? 1 : 0,\n clientConfig.language || 'fr',\n clientConfig.timezone || 'Europe/Berlin',\n clientConfig.brand?.voice || null,\n clientConfig.brand?.target_audience || null,\n clientConfig.brand?.description || null,\n JSON.stringify(clientConfig.hashtags || []),\n JSON.stringify(clientConfig.platform_defaults || {}),\n JSON.stringify(clientConfig.policy || {})\n );\n\n const clientRow = db.prepare('SELECT id FROM clients WHERE slug = ?').get(firstItem.client_slug);\n const clientId = clientRow.id;\n console.log(`[W1] âœ“ Client upserted: ${firstItem.client_slug} (id=${clientId})`);\n\n // Upsert accounts\n const upsertAccount = db.prepare(`\n INSERT INTO accounts (client_id, platform, late_account_id, username, is_default, is_active)\n VALUES (?, ?, ?, ?, ?, 1)\n ON CONFLICT(client_id, platform, late_account_id) DO UPDATE SET username = excluded.username, is_default = excluded.is_default\n `);\n\n function insertAccounts(platform, accountsConfig) {\n if (!accountsConfig) return;\n const accounts = Array.isArray(accountsConfig) ? accountsConfig : [accountsConfig];\n for (const acc of accounts) {\n if (acc.late_account_id) {\n upsertAccount.run(clientId, platform, acc.late_account_id, acc.username || null, acc.is_default ? 1 : 0);\n console.log(`[W1] âœ“ Account upserted: ${platform} - ${acc.username || acc.late_account_id}`);\n }\n }\n }\n\n insertAccounts('instagram', clientConfig.accounts?.instagram);\n insertAccounts('tiktok', clientConfig.accounts?.tiktok);\n\n // Upsert batch\n const upsertBatch = db.prepare(`\n INSERT INTO batches (client_id, slug, name, description, brief, hashtags, schedule_config, folder_path, status)\n VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'processing')\n ON CONFLICT(client_id, slug) DO UPDATE SET\n name = excluded.name, description = excluded.description, brief = excluded.brief, hashtags = excluded.hashtags,\n schedule_config = excluded.schedule_config, folder_path = excluded.folder_path, status = 'processing'\n `);\n\n const batchFolder = firstItem.file_path.split('/').slice(0, -2).join('/');\n\n upsertBatch.run(\n clientId,\n firstItem.batch_name,\n batchConfig.name || firstItem.batch_name,\n batchConfig.description || null,\n batchConfig.brief || null,\n JSON.stringify(batchConfig.hashtags || []),\n JSON.stringify(batchConfig.schedule || {}),\n batchFolder\n );\n\n const batchRow = db.prepare('SELECT id FROM batches WHERE client_id = ? AND slug = ?').get(clientId, firstItem.batch_name);\n const batchId = batchRow.id;\n console.log(`[W1] âœ“ Batch upserted: ${firstItem.batch_name} (id=${batchId})`);\n\n // Upsert content items (v15: added image_description, description_generated_at)\n const upsertItem = db.prepare(`\n INSERT INTO content_items (\n content_id, client_id, batch_id, media_type, file_name, file_path, file_hash, file_size, media_url, preview_url,\n frames_used, cover_path, story_path, scheduled_date, scheduled_time, schedule_at, timezone, slot, platforms,\n image_description, description_generated_at,\n caption_ig, caption_tt, caption_override, hashtags_final, status, error_message, retry_count,\n late_media_id, late_media_url, late_post_id, instagram_account_id, tiktok_account_id, notes, fingerprint, ingest_id, validated_at\n ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n ON CONFLICT(content_id) DO UPDATE SET\n file_hash = excluded.file_hash, file_size = excluded.file_size, media_url = excluded.media_url, preview_url = excluded.preview_url,\n frames_used = excluded.frames_used, cover_path = excluded.cover_path, story_path = excluded.story_path,\n scheduled_date = excluded.scheduled_date, scheduled_time = excluded.scheduled_time, schedule_at = excluded.schedule_at,\n platforms = excluded.platforms, hashtags_final = excluded.hashtags_final,\n image_description = COALESCE(excluded.image_description, content_items.image_description),\n description_generated_at = COALESCE(excluded.description_generated_at, content_items.description_generated_at),\n status = CASE WHEN content_items.status IN ('SCHEDULED', 'PUBLISHED') THEN content_items.status ELSE excluded.status END,\n error_message = excluded.error_message, fingerprint = excluded.fingerprint, ingest_id = excluded.ingest_id, validated_at = excluded.validated_at\n `);\n\n let insertedCount = 0;\n let updatedCount = 0;\n let descriptionsGenerated = 0;\n\n const insertMany = db.transaction((items) => {\n for (const item of items) {\n const d = item.json;\n const existing = db.prepare('SELECT id FROM content_items WHERE content_id = ?').get(d.content_id);\n\n upsertItem.run(\n d.content_id, clientId, batchId, d.media_type, d.file_name, d.file_path, d.file_hash || null, d.file_size || 0,\n d.media_url || null, d.preview_url || null, d.frames_used || null, d.cover_path || null, d.story_path || null,\n d.scheduled_date, d.scheduled_time, d.schedule_at, d.timezone || 'Europe/Berlin', d.slot || 'feed', d.platforms || 'ig',\n d.image_description || null, d.description_generated_at || null,\n d.caption_ig || null, d.caption_tt || null, d.caption_override || null, d.hashtags_final || null,\n d.status || 'PENDING', d.error_message || null, d.retry_count || 0,\n d.late_media_id || null, d.late_media_url || null, d.late_post_id || null,\n d.instagram_account_id || null, d.tiktok_account_id || null, d.notes || null, d.fingerprint || null, d.ingest_id || null, d.validated_at || null\n );\n\n if (existing) updatedCount++;\n else insertedCount++;\n \n if (d.image_description && d.image_description.length > 0) descriptionsGenerated++;\n }\n });\n\n insertMany(items);\n console.log(`[W1] âœ“ Content items: ${insertedCount} inserted, ${updatedCount} updated`);\n console.log(`[W1] âœ“ Image descriptions: ${descriptionsGenerated} generated`);\n\n // Audit log\n const insertAudit = db.prepare(`INSERT INTO audit_log (entity_type, entity_id, action, new_value) VALUES ('batch', ?, 'ingest', ?)`);\n insertAudit.run(batchId, JSON.stringify({ \n ingest_id: firstItem.ingest_id, \n items_inserted: insertedCount, \n items_updated: updatedCount, \n total_items: items.length,\n descriptions_generated: descriptionsGenerated\n }));\n console.log('[W1] âœ“ Audit log entry created');\n\n} finally {\n if (db) db.close();\n}\n\nreturn items.map(item => {\n const { _db_path, _client_config, _batch_config, ...clean } = item.json;\n return { json: clean };\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3160,
        0
      ],
      "id": "9a5807fc-7141-4489-9631-f1584e3e6d5a",
      "name": "Upsert to SQLite"
    },
    {
      "parameters": {
        "jsCode": "/**\n * W1 v17: Batch Report\n * Generates final ingest report and writes job status\n * v15.2: Added progress cleanup\n */\nconst fs = require('fs');\nconst items = $input.all();\n\n// Handle early error exit\nif (items.length === 1 && items[0].json._error) {\n const data = items[0].json;\n const duration = ((Date.now() - data._start_time) / 1000).toFixed(1);\n\n // Write job status (failed)\n const jobPath = `${data._config_base}/active_job.json`;\n try {\n let jobs = { current: {}, executions: {} };\n if (fs.existsSync(jobPath)) jobs = JSON.parse(fs.readFileSync(jobPath, 'utf8'));\n jobs.executions = jobs.executions || {};\n jobs.executions.W1 = {\n last_run: new Date().toISOString(),\n status: 'failed',\n client: data.client || null,\n batch: data.batch || null,\n duration_ms: Date.now() - data._start_time,\n error_code: data.error_code,\n error_message: data.error_message\n };\n fs.writeFileSync(jobPath, JSON.stringify(jobs, null, 2));\n console.log('[W1] âœ“ Job status written (failed)');\n } catch (e) {\n console.log(`[W1] âš  Could not write job status: ${e.message}`);\n }\n\n console.log('[W1] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');\n console.log(`[W1] âœ— FAILED: ${data.error_code}`);\n console.log(`[W1] ${data.error_message}`);\n console.log('[W1] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');\n\n return [{\n json: {\n success: false,\n workflow: 'W1',\n version: '16',\n error_code: data.error_code,\n error_message: data.error_message,\n client: data.client || null,\n batch: data.batch || null,\n duration_seconds: parseFloat(duration),\n generated_at: new Date().toISOString()\n }\n }];\n}\n\nconst configNode = $('Load Config (Database)').first().json;\nconst startTime = configNode._start_time || Date.now();\nconst duration = ((Date.now() - startTime) / 1000).toFixed(1);\n\n// Collect per-item errors and stats\nconst itemErrors = [];\nlet descriptionsGenerated = 0;\nfor (const item of items) {\n if (item.json._item_errors?.length > 0) {\n for (const err of item.json._item_errors) {\n itemErrors.push({ content_id: item.json.content_id, file: item.json.file_name, code: err.code, message: err.message });\n }\n }\n if (item.json.image_description && item.json.image_description.length > 0) {\n descriptionsGenerated++;\n }\n}\n\nconst blockedCount = items.filter(i => i.json.status === 'BLOCKED').length;\nconst hasErrors = blockedCount > 0;\n\nconst report = {\n success: !hasErrors,\n workflow: 'W1',\n version: '16',\n generated_at: new Date().toISOString(),\n duration_seconds: parseFloat(duration),\n client: configNode.client_slug || 'unknown',\n batch: configNode.batch_name || 'unknown',\n source: configNode._source || 'unknown',\n ingest_id: configNode._ingest_id,\n ...(hasErrors && { error_code: 'VALIDATION_FAILED', error_message: `${blockedCount} items blocked` }),\n summary: {\n total: items.length,\n ready_for_ai: items.filter(i => i.json.status === 'NEEDS_AI').length,\n ready_for_review: items.filter(i => i.json.status === 'NEEDS_REVIEW').length,\n blocked: blockedCount,\n image_descriptions: descriptionsGenerated\n },\n by_type: {\n photos: items.filter(i => i.json.media_type === 'photo').length,\n videos: items.filter(i => i.json.media_type === 'video').length\n },\n schedule_info: {\n start_date: items.length > 0 ? items[0].json.scheduled_date : null,\n end_date: items.length > 0 ? items[items.length - 1].json.scheduled_date : null,\n total_days: items.length > 0 ? Math.ceil((new Date(items[items.length - 1].json.scheduled_date) - new Date(items[0].json.scheduled_date)) / (1000 * 60 * 60 * 24)) + 1 : 0\n },\n optimization: {\n vlm_resize_enabled: configNode._vlm_resize_enabled,\n vlm_resize_max: configNode._vlm_resize_max,\n vlm_resize_quality: configNode._vlm_resize_quality\n },\n errors: itemErrors,\n blocked_items: items.filter(i => i.json.status === 'BLOCKED').map(i => ({ content_id: i.json.content_id, file: i.json.file_name, error: i.json.error_message })),\n next_steps: []\n};\n\nif (report.summary.blocked > 0) report.next_steps.push(`Fix ${report.summary.blocked} blocked items`);\nif (report.summary.ready_for_ai > 0) report.next_steps.push(`Run W2 to generate AI captions for ${report.summary.ready_for_ai} items`);\nif (report.summary.ready_for_review > 0) report.next_steps.push(`${report.summary.ready_for_review} items have manual captions - review in UI`);\n\n// Write job status\nconst jobPath = `${configNode._config_base}/active_job.json`;\ntry {\n let jobs = { current: {}, executions: {} };\n if (fs.existsSync(jobPath)) jobs = JSON.parse(fs.readFileSync(jobPath, 'utf8'));\n jobs.executions = jobs.executions || {};\n jobs.executions.W1 = {\n last_run: new Date().toISOString(),\n status: hasErrors ? 'partial' : 'success',\n client: report.client,\n batch: report.batch,\n ingest_id: report.ingest_id,\n duration_ms: Date.now() - startTime,\n summary: report.summary,\n ...(itemErrors.length > 0 && { errors: itemErrors })\n };\n fs.writeFileSync(jobPath, JSON.stringify(jobs, null, 2));\n console.log('[W1] âœ“ Job status written');\n} catch (e) {\n console.log(`[W1] âš  Could not write job status: ${e.message}`);\n}\n\n// v15.2: Clear ingest progress\ntry {\n const Database = require('better-sqlite3');\n const db = new Database(configNode.db_path);\n const clientRow = db.prepare('SELECT id FROM clients WHERE slug = ?').get(configNode.client_slug);\n if (clientRow) {\n db.prepare('UPDATE batches SET ingest_progress = NULL WHERE client_id = ? AND slug = ?')\n .run(clientRow.id, configNode.batch_name);\n console.log('[W1] âœ“ Ingest progress cleared');\n }\n db.close();\n} catch (e) {\n console.log(`[W1] âš  Could not clear progress: ${e.message}`);\n}\n\nconsole.log('[W1] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');\nconsole.log(`[W1] ${hasErrors ? 'âš ' : 'âœ“'} Ingest complete: ${report.summary.total} items`);\nconsole.log(`[W1] Ready for AI: ${report.summary.ready_for_ai}`);\nconsole.log(`[W1] Ready for review: ${report.summary.ready_for_review}`);\nconsole.log(`[W1] Blocked: ${report.summary.blocked}`);\nconsole.log(`[W1] Image descriptions: ${report.summary.image_descriptions}`);\nconsole.log('[W1] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');\n\nreturn [{ json: report }];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        3380,
        0
      ],
      "id": "7c3bd6a9-e08f-412d-bd0c-4b2eac1b75d4",
      "name": "Batch Report"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.5,
      "position": [
        3600,
        112
      ],
      "id": "756ff6b9-52b6-4207-88fc-aa46ee381eaa",
      "name": "Webhook Response"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-ready-for-ai",
              "leftValue": "={{ $json.summary?.ready_for_ai }}",
              "rightValue": 0,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        3380,
        -112
      ],
      "id": "check-trigger-w2",
      "name": "Has Items for AI?"
    }
  ],
  "connections": {
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Load Config (Database)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Context": {
      "main": [
        [
          {
            "node": "Auto-Discover & Schedule",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Auto-Discover & Schedule": {
      "main": [
        [
          {
            "node": "Validate Media",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Media": {
      "main": [
        [
          {
            "node": "Prepare Image for VLM",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Image for VLM": {
      "main": [
        [
          {
            "node": "Skip VLM?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Skip VLM?": {
      "main": [
        [
          {
            "node": "Merge VLM Results",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Resize for VLM",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Resize for VLM": {
      "main": [
        [
          {
            "node": "Build VLM Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build VLM Request": {
      "main": [
        [
          {
            "node": "Call VLM",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call VLM": {
      "main": [
        [
          {
            "node": "Handle VLM Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Handle VLM Response": {
      "main": [
        [
          {
            "node": "Needs Retry?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Needs Retry?": {
      "main": [
        [
          {
            "node": "Prepare VLM Retry",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Merge VLM Results",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Prepare VLM Retry": {
      "main": [
        [
          {
            "node": "Call VLM",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge VLM Results": {
      "main": [
        [
          {
            "node": "Clean for DB",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Clean for DB": {
      "main": [
        [
          {
            "node": "Upsert to SQLite",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upsert to SQLite": {
      "main": [
        [
          {
            "node": "Batch Report",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Batch Report": {
      "main": [
        [
          {
            "node": "Webhook Response",
            "type": "main",
            "index": 0
          },
          {
            "node": "Has Items for AI?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Items for AI?": {
      "main": [
        []
      ]
    },
    "Load Config (Database)": {
      "main": [
        [
          {
            "node": "Validate Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": true
  },
  "pinData": {},
  "active": true,
  "meta": {},
  "tags": [],
  "versionId": "e616177f-9a58-4a8a-80d1-f1b99eb9c9b4"
}